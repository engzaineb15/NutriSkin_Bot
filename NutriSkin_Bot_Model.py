# -*- coding: utf-8 -*-
"""impact-of-vitamin-deficiencies-on-skin-diseases (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QGwrpoEECEJGQkNV9xsaGw5dcFEq62Oq
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings(action="ignore")

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import sklearn.metrics as metrics

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from tensorflow.keras.applications import ResNet152
from sklearn.utils.class_weight import compute_class_weight

classes = [
    "Acne and Rosacea Photos",
    "Atopic Dermatitis Photos",
    "Eczema Photos",
    "Light Diseases and Disorders of Pigmentation",
    "Psoriasis pictures Lichen Planus and related diseases"
]
n_classes = len(classes)

# Load training and testing datasets from the specified directories
train_dir = '/kaggle/input/dermnet/train/'
test_dir = '/kaggle/input/dermnet/test/'

def load_dataset(directory):

    """
    Loads image file paths and their corresponding class labels into a DataFrame.

    Parameters:
    directory (str): Path to the dataset directory.

    Returns:
    pd.DataFrame: A shuffled DataFrame containing image file paths and their associated class labels.
    """
    dataset_df = pd.DataFrame(columns=['path', 'class'])  # Initialize an empty DataFrame

    for y_class in classes:
        dir__ = os.path.join(directory, y_class)  # Construct class directory path
        if os.path.exists(dir__):  # Check if the directory exists
            imgs_dir = os.listdir(dir__)  # List all image files in the directory
            imgs_dir = [os.path.join(dir__, img) for img in imgs_dir]  # Get full paths of images
            df = pd.DataFrame({'path': imgs_dir, 'class': y_class})  # Create a temporary DataFrame
            dataset_df = pd.concat([dataset_df, df], axis=0)  # Append data to the main DataFrame

    # Shuffle the dataset to ensure randomization and reset the index
    return dataset_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Load training and testing datasets into Pandas DataFrames
train_df = load_dataset(train_dir)
test_df = load_dataset(test_dir)

from sklearn.utils.class_weight import compute_class_weight
import numpy as np


if train_df.empty:
    raise ValueError("Error: Training dataframe is empty. Check dataset path.")

if 'class' not in train_df.columns:
    raise ValueError("Error: Column 'class' is missing from train_df.")


# Convert class names to numbers so that `compute_class_weight` works properly

class_mapping = {cls: i for i, cls in enumerate(classes)}
train_df['class_encoded'] = train_df['class'].map(class_mapping)


# Calculate class weights to prevent model bias

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_df['class_encoded']),
    y=train_df['class_encoded'].to_numpy()
)


# Convert `class_weights` to a dictionary
class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}

print("Computed Class Weights:", class_weights_dict)

"""

```
# This is formatted as code
```

# Model (ResNet 152)"""

# Data augmentation for training images to improve model generalization
train_datagen = ImageDataGenerator(
    rescale=1./255,            # Normalize pixel values to the range [0, 1]
    rotation_range=40,         # Randomly rotate images up to 40 degrees
    width_shift_range=0.3,     # Randomly shift images horizontally by 30% of width
    height_shift_range=0.3,    # Randomly shift images vertically by 30% of height
    shear_range=0.3,           # Apply shear transformation
    zoom_range=0.3,            # Random zoom in/out by 30%
    brightness_range=[0.8, 1.2], # Randomly adjust brightness between 80% and 120%
    horizontal_flip=True,      # Flip images horizontally
    channel_shift_range=30,    # Randomly adjust color channels to enhance diversity
    fill_mode='nearest'        # Fill missing pixels after transformations
)

# Only rescaling for test/validation data (no augmentation to keep consistency)
test_datagen = ImageDataGenerator(rescale=1./255)

def create_data_generator(df, datagen):
    """
    Creates a data generator from a given DataFrame.

    Parameters:
    - df: DataFrame containing image file paths and labels.
    - datagen: ImageDataGenerator instance for augmentation/preprocessing.

    Returns:
    - A generator that yields batches of (image, label) pairs.
    """
    return datagen.flow_from_dataframe(
        df, x_col='path', y_col='class', target_size=(224, 224),
        batch_size=32, class_mode='categorical'
    )

# Generate training and test datasets with respective preprocessing
train_generator = create_data_generator(train_df, train_datagen)
test_generator = create_data_generator(test_df, test_datagen)

# Load the pre-trained ResNet152 model with ImageNet weights.
# We exclude the top classification layer since we will add our own custom classifier.
base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Enable fine-tuning by making the base model trainable.
base_model.trainable = True

# Freeze all layers except the last 30 layers to retain pre-trained knowledge while allowing some fine-tuning.
for layer in base_model.layers[:-30]:
    layer.trainable = False

# ðŸ“Œ Build the final classification model
model = keras.Sequential([
    base_model,  # Pre-trained feature extractor (ResNet152)
    layers.GlobalAveragePooling2D(),  # Convert feature maps into a 1D vector
    layers.Dense(512, activation='relu'),  # Fully connected layer with 512 neurons
    layers.BatchNormalization(),  # Normalize activations to improve training stability
    layers.Dropout(0.4),  # Dropout to prevent overfitting
    layers.Dense(256, activation='relu'),  # Additional dense layer for feature learning
    layers.BatchNormalization(),  # Another Batch Normalization for stability
    layers.Dropout(0.4),  # Another Dropout layer to improve generalization
    layers.Dense(n_classes, activation='softmax')  # Output layer with softmax activation for multi-class classification
])

# ðŸ“Œ One Cycle Learning Rate Schedule
# This function adjusts the learning rate dynamically during training.
# It starts with a high learning rate, then gradually decreases it for stable convergence.
def one_cycle_lr(epoch, lr):
    max_lr = 0.001  # Maximum learning rate
    min_lr = 1e-6  # Minimum learning rate
    total_epochs = 20  # Total number of training epochs

    if epoch < total_epochs * 0.3:
        return max_lr  # Initial phase: High learning rate
    elif epoch < total_epochs * 0.7:
        return max_lr / 10  # Mid-phase: Moderate learning rate
    else:
        return min_lr  # Final phase: Very low learning rate to stabilize training

# Compile the model with the Adam optimizer and categorical cross-entropy loss.
# - Adam optimizer (learning rate = 0.0005) is used for adaptive learning.
# - Categorical cross-entropy is chosen as the loss function since this is a multi-class classification problem.
# - Accuracy is used as the evaluation metric to measure model performance.
model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define callbacks to improve model training and prevent overfitting
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    # Stops training if validation loss does not improve for 5 consecutive epochs
    # Restores the best model weights to prevent overfitting

    LearningRateScheduler(one_cycle_lr)
    # Adjusts the learning rate dynamically using the One Cycle Learning Rate policy
    # Helps the model converge faster and avoid local minima
]

# Train the model with the specified parameters
# - train_generator: The training dataset with augmented images
# - validation_data: The validation dataset to evaluate model performance
# - epochs: Number of times the model will go through the entire dataset (20 epochs)
# - class_weight: Balances the class distribution to prevent bias toward dominant classes
# - callbacks: Includes early stopping and learning rate adjustments to optimize training

model.fit(
    train_generator,
    validation_data=test_generator,
    epochs=20,
    class_weight=class_weights_dict,
    callbacks=callbacks
)