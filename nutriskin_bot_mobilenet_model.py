# -*- coding: utf-8 -*-
"""NutriSkin_Bot_MobileNet_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wa3YKCY7cbvNQyN-GPdLT7GSyc5Bou_C
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings(action="ignore")

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import sklearn.metrics as metrics

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2

classes = [
    "Acne and Rosacea Photos",  # نقص فيتامين A وE والزنك
    "Atopic Dermatitis Photos",  # نقص فيتامين D وB6 وB12
    "Eczema Photos",  # نقص فيتامين D وE والأوميغا 3
    "Light Diseases and Disorders of Pigmentation",  # نقص فيتامين B12 وC وD
    "Psoriasis pictures Lichen Planus and related diseases"  # نقص فيتامين D والأوميغا 3
]
n_classes = len(classes)

# Load training dataset
train_dir = r'/kaggle/input/dermnet/train/'
dermnet_train_df = pd.DataFrame(columns=['path', 'class'])

for y_class in classes:
    dir__ = os.path.join(train_dir, y_class)
    if os.path.exists(dir__):
        imgs_dir = os.listdir(dir__)
        imgs_dir = [os.path.join(dir__, img) for img in imgs_dir]
        df = pd.DataFrame({'path': imgs_dir, 'class': y_class})
        dermnet_train_df = pd.concat([dermnet_train_df, df], axis=0)
    else:
        print(f"Directory not found: {dir__}")

# Shuffle the training dataset
dermnet_train_df = dermnet_train_df.sample(frac=1, random_state=42).reset_index(drop=True)
print(dermnet_train_df)

# Load testing dataset
test_dir = r'/kaggle/input/dermnet/test/'
dermnet_test_df = pd.DataFrame(columns=['path', 'class'])

for y_class in classes:
    dir__ = os.path.join(test_dir, y_class)
    if os.path.exists(dir__):
        imgs_dir = os.listdir(dir__)
        imgs_dir = [os.path.join(dir__, img) for img in imgs_dir]
        df = pd.DataFrame({'path': imgs_dir, 'class': y_class})
        dermnet_test_df = pd.concat([dermnet_test_df, df], axis=0)
    else:
        print(f"Directory not found: {dir__}")

# Shuffle the testing dataset
dermnet_test_df = dermnet_test_df.sample(frac=1, random_state=42).reset_index(drop=True)
print(dermnet_test_df)

# Data preparation
batch_size = 32
img_size = (224, 224)
data_dir = train_dir

datagen = ImageDataGenerator(
    validation_split=0.2,
    # rescale=1./255,
    # rotation_range=30,
    # width_shift_range=0.2,
    # height_shift_range=0.2,
    # shear_range=0.2,
    # zoom_range=0.2,
    # horizontal_flip=True,
    preprocessing_function=tf.keras.applications.mobilenet_v3.preprocess_input,
    # brightness_range=[0.8, 1.2]
)

# Training data generator
train_generator = datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training',
    classes=classes
)

# Validation data generator
val_generator = datagen.flow_from_directory(
    test_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation',
    classes=classes
)

test_gen=datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    classes=classes
)

IMG_SIZE=244

classes=list(train_generator.class_indices.keys())
print (classes)





num_classes = len(classes)

#Callback to save the best model. Using checkpoint and earlystopping to monitor validation accuracy
callbacks_list = [
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6),

    tf.keras.callbacks.ModelCheckpoint(
        filepath='mbNetV2model2.keras',
        monitor='val_accuracy', save_best_only=True, verbose=1),
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,verbose=1)
]

from sklearn.utils import class_weight
import numpy as np
train_labels = train_generator.classes

# Number of classes
num_classes = len(classes)


# Extract labels directly from the train_dataset
train_labels = train_generator.classes

# Calculate class weights
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels),
    y=train_labels
)

# Convert class weights to dictionary
class_weights_dict = dict(enumerate(class_weights))

# Print class weights for reference
print("Class weights:", class_weights_dict)

from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.applications import MobileNetV3Large
from tensorflow.keras.models import Model



# Load the MobileNetV2 model pre-trained on ImageNet
#base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
local_weights_path = "/kaggle/input/mobilenetv3weights/weights_mobilenet_v3_large_224_1.0_float_no_top_v2 (1).h5"


base_model = MobileNetV3Large(weights=None, include_top=False, input_shape=(224, 224, 3))
base_model.load_weights(local_weights_path)

#Freeze the early layers of the base model
for layer in base_model.layers[:10]:
    layer.trainable = False

# Build a new model on top of the pre-trained base
inputs = base_model.input
x = base_model(inputs, training=False)
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(num_classes, activation='softmax')(x)

model = Model(inputs, outputs)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])


# Train the model
history = model.fit(train_generator,
                    epochs=50,
                    validation_data=val_generator,
                    # class_weight=class_weights_dict,
                    callbacks=[callbacks_list])

#continue training if needed
#history2 = model.fit(
 #   train_generator,
  # initial_epoch=50,
  #  validation_data=val_generator,
   # callbacks=[callbacks_list]
#)

test_loss, test_accuracy = model.evaluate(test_gen)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

model.load_weights('mbNetV2model2.keras')

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc='upper left')
plt.show()

